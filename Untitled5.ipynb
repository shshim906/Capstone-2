{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist=os.listdir(\"/home/seo/ISIC DATA\")\n",
    "meta=[file for file in filelist if not file.endswith(\".jpeg\") and not file.endswith(\".png\") and file.startswith(\"ISIC\")]\n",
    "pics=[file for file in filelist if (file.endswith(\".jpeg\") or file.endswith(\".png\")) and file.startswith(\"ISIC\")]\n",
    "masklist=os.listdir(\"/home/seo/Masks\")\n",
    "masks=[file for file in masklist if file.endswith(\".png\") and file.startswith(\"ISIC\")]\n",
    "mask2=[file.split('_segmentation')[0] for file in masklist if file.endswith(\".png\") and file.startswith(\"ISIC\")]\n",
    "test_pics=[file for file in pics if file.split('.jpeg')[0] in mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "IMG_PATH = \"/home/seo/ISIC DATA\"\n",
    "MASK_PATH = \"/home/seo/Masks\"\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test IDs\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "test_ids = next(os.walk(TEST_PATH))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 128, 128, 3), dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing train images and masks ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing test images ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get and resize train images and masks\n",
    "images = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "labels = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    img = imread(\"/home/seo/ISIC DATA/\"+str(id_))[:,:,:IMG_CHANNELS]\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    images[n] = img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "        mask_ = imread(\"/home/seo/Masks/\"+id_.split('.')[0]+'_segmentation.png')\n",
    "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                      preserve_range=True), axis=-1)\n",
    "        mask = np.maximum(mask, mask_)\n",
    "    labels[n] = mask\n",
    "\n",
    "X_train = images\n",
    "Y_train = labels\n",
    "\n",
    "# Get and resize test images\n",
    "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "sizes_test = []\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = TEST_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    sizes_test.append([img.shape[0], img.shape[1]])\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_test[n] = img\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle():\n",
    "    global images, labels\n",
    "    p = np.random.permutation(len(X_train))\n",
    "    images = X_train[p]\n",
    "    labels = Y_train[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_s, iters):\n",
    "    if(iters == 0):\n",
    "        shuffle()\n",
    "    count = batch_s * iters\n",
    "    return images[count:(count + batch_s)], labels[count:(count + batch_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv2d(input_tensor, filter_size, output_size, out_channels, in_channels, name, strides = [1, 1, 1, 1]):\n",
    "    dyn_input_shape = tf.shape(input_tensor)\n",
    "    batch_size = dyn_input_shape[0]\n",
    "    out_shape = tf.stack([batch_size, output_size, output_size, out_channels])\n",
    "    filter_shape = [filter_size, filter_size, out_channels, in_channels]\n",
    "    w = tf.get_variable(name=name, shape=filter_shape)\n",
    "    h1 = tf.nn.conv2d_transpose(input_tensor, w, out_shape, strides, padding='SAME')\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_tensor, depth, kernel, name, strides=(1, 1), padding=\"SAME\"):\n",
    "    return tf.layers.conv2d(input_tensor, filters=depth, kernel_size=kernel, strides=strides, padding=padding, activation=tf.nn.relu, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 128, 128, 3])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 128, 128, 1])\n",
    "lr = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 21:16:04.767572 140620025136960 deprecation.py:323] From <ipython-input-9-d1e26f9169f7>:2: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0714 21:16:04.773575 140620025136960 deprecation.py:506] From /home/seo/environments/my_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0714 21:16:05.172097 140620025136960 deprecation.py:323] From /home/seo/environments/my_env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "net = conv2d(X, 32, 1, \"Y0\") #128\n",
    "\n",
    "net = conv2d(net, 64, 3, \"Y2\", strides=(2, 2)) #64\n",
    "\n",
    "net = conv2d(net, 128, 3, \"Y3\", strides=(2, 2)) #32\n",
    "\n",
    "\n",
    "net = deconv2d(net, 1, 32, 128, 128, \"Y2_deconv\") # 32\n",
    "net = tf.nn.relu(net)\n",
    "\n",
    "net = deconv2d(net, 2, 64, 64, 128, \"Y1_deconv\", strides=[1, 2, 2, 1]) # 64\n",
    "net = tf.nn.relu(net)\n",
    "\n",
    "net = deconv2d(net, 2, 128, 32, 64, \"Y0_deconv\", strides=[1, 2, 2, 1]) # 128\n",
    "net = tf.nn.relu(net)\n",
    "\n",
    "logits = deconv2d(net, 1, 128, 1, 32, \"logits_deconv\") # 128\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(Y_, logits)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training loss: 0.0\n",
      "2 training loss: 0.0\n",
      "3 training loss: 0.0\n",
      "4 training loss: 0.0\n",
      "5 training loss: 0.0\n",
      "6 training loss: 0.0\n",
      "7 training loss: 0.0\n",
      "8 training loss: 0.0\n",
      "9 training loss: 0.0\n",
      "10 training loss: 0.0\n",
      "11 training loss: 0.0\n",
      "12 training loss: 0.0\n",
      "13 training loss: 0.0\n",
      "14 training loss: 0.0\n",
      "15 training loss: 0.0\n",
      "16 training loss: 0.0\n",
      "17 training loss: 0.0\n",
      "18 training loss: 0.0\n",
      "19 training loss: 0.0\n",
      "20 training loss: 0.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_count = 0\n",
    "display_count = 1\n",
    "for i in range(10000):\n",
    "    # training on batches of 10 images with 10 mask images\n",
    "    if(batch_count > 67):\n",
    "        batch_count = 0    \n",
    "\n",
    "    batch_X, batch_Y = next_batch(10, batch_count)\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "    feed_dict = {X: batch_X, Y_: batch_Y, lr: 0.0005}\n",
    "    loss_value, _ = sess.run([loss, optimizer], feed_dict=feed_dict)\n",
    "\n",
    "    if(i % 500 == 0):\n",
    "        print(str(display_count) + \" training loss:\", str(loss_value))\n",
    "        display_count +=1\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1239571c080b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m#random.randint(0, 64) #len(X_test) - 1 = 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "ix = 3 #random.randint(0, 64) #len(X_test) - 1 = 64\n",
    "test_image = X_test[ix].astype(float)\n",
    "imshow(test_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
